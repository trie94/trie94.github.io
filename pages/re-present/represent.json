{
    "title":"Re-Present",
    "thumb":"VR public speaking practice tool",
    "tags":"Oculus Rift & Touch, Kinect, Watson Speech to Text",
    "link":"/re-present",
    "repo":"https://github.com/trie94/re-present",
    "prototype":"https://www.etc.cmu.edu/projects/re-present/",
    "src": "https://www.youtube.com/embed/pJbNACveJJ0?rel=0",
    "summary":"Re-Present is a VR app that helps people enhance their public speaking skills by providing features where they can practice and review their performance.",
    "list_summary":"Re-Present was a semester-long project at the Entertainment Technology Center, Carnegie Mellon University. As a lead programmer in a five-member team, I implemented a play-back feature in the Review mode along with interactions of the Timeline that enables users to navigate between different moments; created a modular system that is capable of collecting and compiling voice, proxy eye contact, and body gestural/ postural data that can be extended to other data sets; made entry-points to interaction designers/ prototypers to test their interaction design. Click here to see more!",
    "duration":"15 weeks",
    "role":"Lead Programmer",
    "platform":"Oculus Rift",
    "tool":"Unity, Oculus Rift & Touch controllers, Kinect, and Watson Speech to Text API",
    "section1":"In order to enhance one's presentation skills, it is essential to iterate on a practice-review cycle. However, there are not enough resources that help people review their performances. To unclog this practice-review iteration cycle, our team decided to create a tool that provides an experience where people can not just practice but also review their presentations.",
    "section2": "In this experience, there are three big fragments: There is a Practice mode, a space that provides a classroom setting where users can upload a presentation material as a pdf format and practice their presentation with attentive audience. Once they have finished the Practice mode, following is a Summary page where the users can see their performance results. It includes the number of filler words and the level of eye contact with the audience members. The Summary page is provided before the users enter the Review mode, in order to guide them what components to focus on. Now they enter the Review mode, where they become one of the audience members and review their performance. A timeline tool is provided on the virtual desk to enable users to navigate between different moments.",
    "section3":"In order to better manage data, four types of data managers are used: Trackers, Analyzers, Reflectors, and Renderers.",
    "section4":"Trackers record data from the Practice mode and save it locally as a csv file format. Data sets include head orientation, body gesture/ posture, voice, pdf slide page index, and audience engagement level data. The figure above shows tracker-data pairs:",
    "section5":"Analyzers load the files recorded by the Trackers and make an analysis in order to generate a Summary. The analyzed data is also stored to the local disk under the Summary directory. Following image is one of the examples how the raw data is analyzed. Watson Speech to Text API is used for analyzing the audio data.",
    "section6":"Renderers load the processed data and render it to the Summary page. Summary page is rendered after each practice session, which is shown in VR. PC version Summary can be found in the previous session list page which helps users choose one of the sessions to review. Following example is a Summary page that can be found in the session list.",
    "section7":"Reflectors manage play-back in the Review mode. They load and read through the csv files that are saved by the Trackers and read through csv files.",
    "section8":"Since the tool is planning to be used in a class at Heinz School, Carnegie Mellon University in the upcomming spring semester, we made the system that supports multi-user and multi-session, where more than one user would revisit the app to practice a new session or to review and compare their previous performances. In order to achieve this use case, the session data is stored in the session date directory under the user name.",
    "section9":"Once a user enters the Review mode, the user now becomes one of the audience members and watch oneself as a wooden figure. The gaze cone is added to the figure's head in order to highlight where the user was looking at. There are also floating color balls spread out in the classroom which visualize proxy eye contact level for each audience member. The user can also hear one's voice with a volume bar on the left side, which shows current volume with a average volume mark. A Timeline is provided to help a user better review one's presentation, allowing the user to play, pause, and scrub with a similar fashion when one uses a general video player. It enables the user to focus on specific moments and to navigate between different moments.",
    "section10":"Our team decided to use a curved design for the timeline to make a consistent distance from a user, making it easier to scrub. To achieve the design and the scrubbing feature, a custom shader that renders two different colors based on the progress is used. The timeline model is UV mapped so that 2D coordinate points can be used for calculating colors. When the user aims at the timeline body with a laser from a controller and triggers the button, the scrub event is fired and sends the x value(0-1) of the hit point to the Reflectors. Reflectors are the managers that handle play-back feature in the Review mode.",
    "code1":"if (state == ControllerStatus.Trigger && hit.collider.tag == \"timeline\")\n{\n\tEventManager.TriggerEvent(\"OnScrub\", hit.textureCoord.x);\n}",
    "section11":"The Reflectors subscribe the Scrub event. Following is an example how the audio is updated when the Scrub event is fired.",
    "code2":"void OnScrub(object data)\n{\n\tif (audioSource.time <= 0f)\n\t{\n\t\tcurTime = 0f;\n\t\treturn;\n\t}\n\n\tif (data is float)\n\t{\n\t\tcurTime = (float)data * recordDuration;\n\n\t\tif (curTime >= audioSource.clip.length)\n\t\t{\n\t\t\tcurTime = audioSource.clip.length;\n\t\t}\n\t\taudioSource.time = curTime;\n\t}\n}",
    "section12":"Timeline manager is dependent on the audio file and it updates the progress by setting the value of the _Progress property where it determines the boundary of the two colors in the Timeline shader.",
    "code3":"void Update\n{\n\t&hellip;\n\trend.material.SetFloat(\"_Progress\", curtime / totalTime);\n}",
    "section13":"Timeline manager is dependent on the audio file and it updates the progress by setting the value of the _Progress property where it determines the boundary of the two colors in the Timeline shader.",
    "section14":"Following is a part of the Timeline shader that illustrates how it renders the two colors.",
    "code4":"&hellip;\nProperties {\n\t_ProColor (\"ProgressColor\", Color) = (1,1,1,1)\n\t_Color (\"Color\", Color) = (1,0,0,1)\n\t_MainTex (\"Albedo (RGB)\", 2D) = \"white\" {}\n\t_Progress (\"Progress\", Range(0,1)) = 0.0\n}\n\n\tsampler2D _MainTex;\n\n\tstruct Input {\n\t\tfloat2 uv_MainTex;\n\t};\n\n\t&hellip;\n\tfixed4 _ProColor;\n\tfixed4 _Color;\n\tfloat _Progress;\n\nSubShader {\n\tvoid surf (Input IN, inout SurfaceOutputStandard o) {\n\t\tfixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color;\n\t\tfloat progress = IN.uv_MainTex.x;\n\t\to.Albedo = progress < _Progress ? _ProColor.rgb : _Color.rgb;\n\t\t&hellip;\n\t}\n}",
    "section15":"Since Timeline allows play, pause, and scrub features, the interaction involves not just a Timeline body but also controller and button interactions and interfaces. At first, we used a wand as a visual for the pointer interface that comes out from the index finger of the virtual right hand model. However, Some people felt detached from virtual-self since the hand model was mannish and had a flesh texture and  the hands don't match with their hands. Also, users were confused when they interact with the objects because there was no feedback when a user hover on to it.",
    "section16":"To fix such issues, we add a virtual controller on the right hand and a virtual hand on the left hand. The user can switch the virtual controller and the hand as desire by clicking on the joy stick on the Oculus Touch controller. The rim light shader is used to the hand model to avoid the VR self-detachment.",
    "section17":"The pointer visual has changed into a laser that is coming out from the virtual controller model. This makes interaction more intuitive since the users can make interactions by using it as they use an actual pointer in the real life. Hovering state is also added where the laser is highlighted when it intersects with an interactable object.",
    "section18":"Later, we added a small hit point at the intersection in order to clearly visualize where the user is hitting with the laser. Play and pause buttons are combined into one to make clear which button is interactable. Through out several iteration including polishing timing, size, and color, we were able to create intuitive interactions."
}